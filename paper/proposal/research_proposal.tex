\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

\title{Intent Segregation Cybersecurity Architecture for LLM Systems: \\
Multi-Layer Defense Through Sacrificial Testing and Consensus Voting}
\author{Connor Pham}
\date{\today}

\lstdefinelanguage{Rust}{
    keywords={fn, let, mut, const, static, struct, enum, impl, trait, pub, use, match, if, else, for, while, loop, return, async, await},
    keywordstyle=\color{blue},
    ndkeywords={i32, i64, usize, String, Vec, Option, Result},
    ndkeywordstyle=\color{darkgray},
    identifierstyle=\color{black},
    sensitive=true,
    comment=[l]{//},
    commentstyle=\color{gray}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    morestring=[b]"
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakatwhitespace=true,
    columns=fullflexible,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{red}
}

\begin{document}

\maketitle

\begin{abstract}
This paper proposes a defense-in-depth architecture for protecting LLM-based systems against prompt injection attacks.
The system combines three novel defensive layers: (1) \emph{zero-trust input probing} using sacrificial LLM instances isolated from production systems, (2) \emph{multi-parser consensus voting} across independent LLM backends with trust-weighted confidence thresholds, and (3) \emph{typed execution boundaries} that eliminate free-form LLM calls.
We evaluate against 173+ adversarial payloads across BIPIA, TaskTracker, AgentDojo, and ASB benchmarks, targeting $<5\%$ Attack Success Rate (TIER 1) with $<10\%$ false refusal rate.
This architecture addresses critical gaps in existing defenses by requiring adversaries to simultaneously fool multiple independent parsers while maintaining benign task utility.
\end{abstract}

\section{Introduction}

Large Language Models deployed in agent-based systems face a critical threat: \textit{prompt injection attacks}, where adversarial inputs manipulate LLM behavior at inference time. Unlike traditional cybersecurity where systems enforce boundaries, LLMs are \textit{designed} to process natural language flexibly, making injection attacks difficult to prevent with simple filtering.

\subsection{Problem Statement}
Given user input $u$ potentially containing injection attacks, and a set of allowed actions $A$ with constraints $C$, design a system that:
\begin{enumerate}
    \item Detects corruption before processing reaches production models
    \item Extracts canonical intent $I$ via multiple independent parsers
    \item Requires consensus (multi-dimensional similarity $\geq 95\%$) for auto-approval
    \item Enforces typed execution (no free-form LLM calls)
    \item Achieves Attack Success Rate (ASR) $< 5\%$ against state-of-the-art attacks
    \item Maintains benign utility $> 85\%$ (false refusal rate $< 10\%$)
\end{enumerate}

\subsection{Contributions}
\begin{enumerate}
    \item \textbf{Sacrificial Testing Model}: Zero-trust input probing via isolated LLM sentries before production parsers process data
    \item \textbf{Multi-Parser Consensus Architecture}: Three independent parsers with trust-weighted voting requiring $\geq 95\%$ similarity
    \item \textbf{Typed Execution Layer}: Eliminates free-form LLM calls in execution; all actions invoke typed function dispatch
    \item \textbf{Immutable Audit Ledger}: Enforce append-only logging of full pipeline (input, parsers, votes, action, output)
    \item \textbf{Principled Human Escalation}: Conflict detection and policy violation triggers for human-in-the-loop approval
\end{enumerate}

\section{Literature Review}

\subsection{Prompt Injection Attacks}

Carlini et al.\ (2023) demonstrated that adversarial prompts can override system instructions at inference time. Follow-up work identified several attack classes:

\begin{itemize}
    \item \textbf{Direct Injection}: Explicit adversarial commands (``IGNORE previous instructions...'')
    \item \textbf{Indirect Injection} (BIPIA 2024): Malicious instructions embedded in legitimate content (emails, web pages, agent outputs)---difficult to detect via input validation alone
    \item \textbf{Semantic Substitution}: Unicode obfuscation, encoding attacks, word replacement attacks
    \item \textbf{Jailbreaking}: Role-play framing (``act as a security expert without restrictions''), multi-turn conversation drift, transfer attacks across models
\end{itemize}

\subsection{Existing Defense Mechanisms}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Defense} & \textbf{Static ASR} & \textbf{Adaptive ASR} & \textbf{FRR} & \textbf{Latency} \\
\midrule
Prompt Shields & $\sim 2\%$ & N/A & $\sim 0.8\%$ & $<100$ms \\
SmoothLLM & $<1\%$ & $>90\%$ & $\sim 5\%$ & 1--2s \\
LangChain Policy & Variable & Variable & $\sim 8\%$ & $<500$ms \\
Human-in-loop & $\sim 0\%$ & $\sim 0\%$ & $\sim 20\%$ & 1--24h \\
\bottomrule
\end{tabular}
\caption{Comparative analysis of existing defenses. Adaptive ASR measures robustness under 100+ optimization iterations (RL-based attacks).}
\label{tab:sota}
\end{table}

Key limitations:
\begin{enumerate}
    \item Single-parser systems lack semantic redundancy
    \item High false refusal rates degrade utility
    \item No mechanism for zero-trust input probing before main processing
    \item Existing systems lack immutable audit trails
\end{enumerate}

\subsection{Research Gaps}

\textbf{No system combines:}
\begin{enumerate}
    \item Zero-trust sacrificial testing before production parsers
    \item Multi-parser consensus voting (3+ independent models)
    \item Trust-weighted voting with explicit confidence thresholds
    \item Typed execution boundaries eliminating LLM calls from execution layer
    \item Immutable forensic auditing of full pipeline
    \item Principled escalation to human review based on parser conflicts
\end{enumerate}

\section{Proposed Architecture}

\subsection{System Overview}

The system enforces a \textit{nine-stage security pipeline}:

\begin{enumerate}
    \item \textbf{Sacrificial Testing Layer}: Isolated LLM instances probe input for corruption patterns
    \item \textbf{Parser Ensemble}: 3 LLM parsers extract structured intent in parallel
    \item \textbf{Consensus Voting Module}: Compare parser outputs; compute confidence score
    \item \textbf{Policy Validation Module}: Enforce action constraints and security policies
    \item \textbf{Human Approval System}: Escalate conflicts to human review if confidence is low
    \item \textbf{Intent Generation Module}: Create signed, trusted intent object with full context binding
    \item \textbf{Execution Engine}: Execute via typed function dispatch, never free-form LLM calls
    \item \textbf{Audit Ledger}: Log full pipeline to immutable, append-only audit trail
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{architecture.png}
    \caption{System Architecture}
    \label{fig:placeholder}
\end{figure}

\subsection{Sacrificial Testing Layer (Stage 1)}

\textbf{Innovation:} Before production parsers process input, test on isolated LLM instances with no shared state or connection to main system.

\[
\text{input} \xrightarrow{\text{Isolated LLM Instances}} \text{[test for corruption]} \xrightarrow{\text{Health Monitoring}} \{\text{SAFE, CORRUPTED}\}
\]

The health monitoring system observes sacrificial instances' metrics (response entropy, error rates, latency anomalies) without direct API contact, quarantining poisoned inputs before Stage 2. This achieves zero-trust validation: the main system never processes potentially corrupted data.

\subsection{Multi-Parser Consensus with Trust Weighting (Stages 2--3)}

Instead of single-parser reliance, use three independent backends:

\begin{align}
P_1 &: \text{OpenAI (GPT models)} \quad \text{trust: } 0.80 \\
P_2 &: \text{DeepSeek (API)} \quad \text{trust: } 0.82 \\
P_3 &: \text{Claude (Anthropic)} \quad \text{trust: } 0.87
\end{align}

\textbf{Voting Rule:} Compute multi-dimensional similarity $\text{sim}(I_i, I_j)$ across all parser pairs (structural match, semantic similarity, action agreement). Confidence score:

\[
C = \frac{1}{3} \sum_{(i,j): i < j} \text{sim}(I_i, I_j) \in [0, 1]
\]

\textbf{Approval Logic:}
\begin{itemize}
    \item $C \geq 0.95$: Auto-approve (high confidence)
    \item $0.75 \leq C < 0.95$: Request optional user confirmation
    \item $C < 0.75$: Escalate to human review
\end{itemize}

\textbf{Security Gain:} Attackers must craft payloads that fool \textit{three independent models} simultaneously---exponentially harder than single-parser bypass.

\subsection{Policy Validation Module (Stage 4)}

Validates parsed intent against provider security policies:

\begin{enumerate}
    \item Action $I_a \in \text{allowed\_actions}$
    \item Expertise domain matches authorized areas
    \item Budget constraints respected
    \item Parameter bounds enforced
\end{enumerate}

Failures trigger automatic escalation to human approval.

\subsection{Typed Execution Boundary (Stage 7)}

Unlike systems that invoke free-form LLM calls in the execution layer, the execution engine invokes only typed function dispatch:

\begin{lstlisting}[language=Rust]
match intent.action {
    Action::FindExperts => find_experts(
        topic: intent.topic,
        expertise: intent.expertise,
        budget: intent.budget
    ),
    Action::Summarize => summarize(
        document: intent.document,
        length: intent.length
    ),
    // ... other typed actions
}
\end{lstlisting}

This eliminates the execution layer as an injection attack surface: no LLM prompts are constructed dynamically based on user input.

\subsection{Immutable Audit Ledger (Stage 8)}

All operations are logged to PostgreSQL with database-level constraints preventing UPDATE/DELETE:

\begin{lstlisting}[language=sql]
CREATE TABLE ledger_entries (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMP NOT NULL,
    input_hash BYTEA NOT NULL,
    parser_outputs JSONB NOT NULL,
    vote_confidence FLOAT NOT NULL,
    action JSONB NOT NULL,
    execution_result JSONB NOT NULL
);

-- Enforce immutability at database level
CREATE RULE ledger_no_update AS ON UPDATE
    TO ledger_entries DO INSTEAD NOTHING;
CREATE RULE ledger_no_delete AS ON DELETE
    TO ledger_entries DO INSTEAD NOTHING;
\end{lstlisting}

\section{Evaluation Plan}

\subsection{Datasets}

\begin{enumerate}
    \item \textbf{BIPIA}: 3,000 indirect prompt injection attack samples
    \item \textbf{TaskTracker}: 31,000 samples with position metadata (95\% CI statistical power)
    \item \textbf{AgentDojo}: 100+ scenarios across 4 domains (Google DeepMind)
    \item \textbf{ASB}: 400+ tool scenarios, 27 attack methods (Agent Security Bench)
\end{enumerate}

\subsection{Metrics}

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{TIER 1} & \textbf{TIER 2} & \textbf{TIER 3} \\
\midrule
Attack Success Rate (ASR) & $< 5\%$ & $< 2\%$ & $< 1\%$ \\
False Refusal Rate (FRR) & $< 10\%$ & $< 8\%$ & $< 5\%$ \\
Vault Detection Rate & $> 95\%$ & $> 97\%$ & $> 99\%$ \\
Parser Agreement (benign) & $> 95\%$ & $> 97\%$ & $> 99\%$ \\
Adaptive ASR ($k=100$) & $< 15\%$ & $< 10\%$ & $< 5\%$ \\
\bottomrule
\end{tabular}
\caption{Success criteria across three security tiers. Tier 1 is minimum viable security; Tier 2 is production-grade; Tier 3 is enterprise.}
\label{tab:metrics}
\end{table}

\subsection{Research Questions}

\begin{enumerate}
    \item[RQ1] Does multi-parser consensus ($\geq 95\%$ similarity) improve robustness against prompt injection vs.\ single-parser systems?
    \item[RQ2] What is the sacrificial testing layer's detection rate on BIPIA/TaskTracker? Does it catch injection attempts before production processing?
    \item[RQ3] How does ASR scale under adaptive attacks ($k$-robustness)? Is growth $< 1.5\times$ over 100 iterations?
    \item[RQ4] Does typed execution eliminate escalation to arbitrary LLM calls? What is the residual attack surface?
    \item[RQ5] What is the utility-security tradeoff? Benign task success rate under attack-resistance constraints?
\end{enumerate}

\section{Expected Contributions}

\begin{enumerate}
    \item \textbf{Novel Architecture}: First system combining zero-trust input probing, multi-parser consensus, and typed execution
    \item \textbf{Defense Innovation}: Sacrificial testing as a general pattern for zero-trust LLM input validation
    \item \textbf{Empirical Benchmarking}: Comprehensive evaluation against 173+ adversarial payloads and 4 industry benchmarks
    \item \textbf{Practical System}: Production-ready implementation with $>85\%$ benign utility
    \item \textbf{Scalable Approach}: Parser-agnostic design compatible with any LLM backend (OpenAI, open-source models, etc.)
\end{enumerate}

\section{Timeline}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Phase} & \textbf{Duration} & \textbf{Deliverable} \\
\midrule
Evaluation Setup & - weeks & Dataset integration, baseline metrics \\
Red Team Execution & - weeks & Phase 1--5 attacks, ASR/FRR measurements \\
Comparative Analysis & - weeks & Benchmarks vs.\ state-of-the-art systems \\
Paper Writing & - weeks & Final manuscript and appendices \\
\bottomrule
\end{tabular}
\end{table}

\section{Significance}

This work addresses a critical gap in LLM security by providing a practical, production-ready architecture that combines multiple defensive layers. Unlike existing approaches that rely on single points of failure, this system enforces defense-in-depth principles: zero-trust input testing, multi-parser redundancy, typed execution boundaries, and immutable auditing. The system is designed for real-world deployment in agent-based applications while maintaining sub-2-second latency and >85\% utility on benign tasks.

\begin{thebibliography}{99}

\bibitem{carlini2023} Carlini, N., et al. (2023). Poisoning Web-Scale Training Datasets is Practical. \textit{USENIX Security Symposium}.

\bibitem{greshake2023} Greshake, K., et al. (2023). Not What You've Signed Up For: Security Analysis of Third-Party Password Managers. \textit{USENIX Security Symposium}.

\bibitem{zou2023} Zou, A., et al. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. \textit{arXiv:2307.15043}.

\bibitem{bipia2024} BIPIA (2024). Benchmark for Indirect Prompt Injection Attacks. 3,000 attack samples.

\bibitem{tasktracker2024} TaskTracker (2024). Large-Scale Injection Attack Dataset. 31,000 samples with position analysis.

\bibitem{agentdojo2024} Google DeepMind (2024). AgentDojo: Agent Security Evaluation Framework. 100+ scenarios, 4 domains.

\bibitem{asb2024} ASB (2024). Agent Security Bench: Tool Integration Security Benchmark. 400+ scenarios, 27 attack methods.

\end{thebibliography}

\end{document}
